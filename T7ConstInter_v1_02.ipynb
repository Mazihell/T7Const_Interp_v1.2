{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mazihell/T7Const_Interp_v1.2/blob/main/T7ConstInter_v1_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3to1EOAUHnr",
        "outputId": "f15ea0c8-7667-4f58-84f5-2c0056937158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import nltk\n",
        "import re\n",
        "#tokenizer\n",
        "nltk.download('punkt')\n",
        "#tagger pré treinado em inglês\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "#palavras para exclusão\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Dsh9wKfeUHny"
      },
      "outputs": [],
      "source": [
        "#Artigos em ingles para donwload\n",
        "link1 = Request('https://link.springer.com/article/10.1007/s42979-021-00592-x',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link2 = Request('https://link.springer.com/article/10.1007/s42979-022-01371-y',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link3 = Request('https://www.ibm.com/cloud/learn/neural-networks',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link4 = Request('https://realpython.com/python-ai-neural-network/',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link5 = Request('https://monkeylearn.com/text-classification/',headers={'User-Agent': 'Mozilla/5.0'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "wF8eb74XUHnz"
      },
      "outputs": [],
      "source": [
        "#armazenando da pagina na variavel\n",
        "pagina1 = urlopen(link1).read().decode('utf-8', 'ignore')\n",
        "pagina2 = urlopen(link2).read().decode('utf-8', 'ignore')\n",
        "pagina3 = urlopen(link3).read().decode('utf-8', 'ignore')\n",
        "pagina4 = urlopen(link4).read().decode('utf-8', 'ignore')\n",
        "pagina5 = urlopen(link5).read().decode('utf-8', 'ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "VxAaHZhKUHnz"
      },
      "outputs": [],
      "source": [
        "soup1 = BeautifulSoup(pagina1, \"lxml\")\n",
        "soup2 = BeautifulSoup(pagina2, \"lxml\")\n",
        "soup3 = BeautifulSoup(pagina3, \"lxml\")\n",
        "soup4 = BeautifulSoup(pagina4, \"lxml\")\n",
        "soup5 = BeautifulSoup(pagina5, \"lxml\")\n",
        "#busca pelo ID do que precisa do html\n",
        "texto1 = soup1.find(id=\"Sec5-content\").text\n",
        "texto2 = soup2.find(id=\"Sec2-content\").text\n",
        "texto3 = soup3.find(id=\"layout-section-second\").text\n",
        "texto4 = soup4.find(id=\"artificial-intelligence-overview\").text\n",
        "texto5 = soup5.find(id=\"___gatsby\").text  \n",
        "#soup3.find_all(id=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "vaaP1V3ZUHn0"
      },
      "outputs": [],
      "source": [
        "dados = [texto1, texto2, texto3, texto4, texto5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "pvAQ6J4sUHn1"
      },
      "outputs": [],
      "source": [
        "# criando os objetos com as expressões regulares\n",
        "remove_espec_carac = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "remove_symb = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "# criando um objeto para remoção de stopwords no idioma ingles\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "# criando a função\n",
        "def text_prepare(text):\n",
        "    \n",
        "    # normalizando texto em letras minúsculas\n",
        "    text = text.lower()\n",
        "    \n",
        "    # substituindo caracteres especiais por espaços em branco\n",
        "    text = remove_espec_carac.sub(' ', text)\n",
        "    \n",
        "    # retornando apenas letras e números\n",
        "    text = remove_symb.sub('', text)\n",
        "    \n",
        "    # removendo as stopwords\n",
        "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
        "    \n",
        "    # retornando o texto modificado\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "UifBO3EFUHn3"
      },
      "outputs": [],
      "source": [
        "# aplicando a função no texto (pre processamento do texto)\n",
        "texto = [text_prepare(x) for x in dados]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "OED8fjFNUHn5"
      },
      "outputs": [],
      "source": [
        "#histograma (quantidade de vezes que determinada palavra aparece no texto)\n",
        "word2count = {}\n",
        "for data in texto:\n",
        "    words = nltk.word_tokenize(data)\n",
        "    for word in words:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "wuA8XbtlUHn7"
      },
      "outputs": [],
      "source": [
        "# biblioteca para ordenação\n",
        "import heapq\n",
        "\n",
        "# ordena a lista para saber qual é a palavra que mais se repete\n",
        "freq_words = heapq.nlargest(10000,word2count, key=word2count.get)\n",
        "#print (freq_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#criando o bag\n",
        "import numpy as np\n",
        "x = []\n",
        "for data in texto:\n",
        "    vector = []\n",
        "    for word in freq_words:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            vector.append(1) # se a palavra está contida no documento coloca-se 1\n",
        "        else:\n",
        "            vector.append(0) # se a palavra não está contida no documento coloca-se 0\n",
        "    x.append(vector)\n",
        "    \n",
        "y = np.asarray(x)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WERgsq_7W04k",
        "outputId": "0a6efeb9-0bff-49cd-949e-b54f3111f3e3"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 1 0 ... 0 0 0]\n",
            " [1 1 1 ... 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "CYhLo86_UHn8"
      },
      "outputs": [],
      "source": [
        "# criando o IDF\n",
        "word_idfs = {}\n",
        "for word in freq_words:\n",
        "    doc_count = 0 \n",
        "    for data in texto:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            doc_count += 1\n",
        "    word_idfs[word] = np.log((len(texto)/doc_count)+1)\n",
        "\n",
        "#deixei comentado o print professor, porque ficou muito grande a quantidade de item na tela.\n",
        "#print (word_idfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "bxRQ5rL2UHn9"
      },
      "outputs": [],
      "source": [
        "# criando o TF\n",
        "tf_matrix = {}\n",
        "\n",
        "for word in freq_words:\n",
        "    doc_tf = []\n",
        "    for data in texto:\n",
        "        frequency = 0\n",
        "        for w in nltk.word_tokenize(data):\n",
        "            if w == word:\n",
        "                frequency += 1\n",
        "        tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "        doc_tf.append(tf_word)\n",
        "    tf_matrix[word] = doc_tf\n",
        "\n",
        "#print (tf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "V0i4m58QUHn-"
      },
      "outputs": [],
      "source": [
        "# TF-IDF cálculo\n",
        "tfidf_matrix = []\n",
        "\n",
        "for word in tf_matrix.keys():\n",
        "    tfidf = []\n",
        "    for value in tf_matrix[word]:\n",
        "        score = value * word_idfs[word]\n",
        "        tfidf.append(score)\n",
        "    tfidf_matrix.append(tfidf)\n",
        "    \n",
        "#print (tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ7ySW3mUHn-",
        "outputId": "d2d25759-6025-46bb-e7a6-cd2506e8e204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nome das variáveis n ['000' '02' '048' ... 'zeror' 'zerovariance' 'zip'] \n",
            "\n",
            "Matrix n (5, 2926) n [[0.         0.         0.         ... 0.0064832  0.0064832  0.        ]\n",
            " [0.         0.         0.02289695 ... 0.         0.         0.        ]\n",
            " [0.         0.02042147 0.         ... 0.         0.         0.02042147]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.01291697 0.         0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer()\n",
        "matrix= vec.fit_transform(texto)\n",
        "print(\"Nome das variáveis n\",vec.get_feature_names_out(),'\\n')\n",
        "print(\"Matrix n\",matrix.shape,\"n\",matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sklearn cossine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "xHtGc4UCoS01",
        "outputId": "93f0f75c-2261-4d52-91e3-42e109e1f3cd"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        000        02       048       081     0once        10       100  \\\n",
              "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.005231  0.006483   \n",
              "1  0.000000  0.000000  0.022897  0.022897  0.000000  0.018473  0.000000   \n",
              "2  0.000000  0.020421  0.000000  0.000000  0.020421  0.000000  0.000000   \n",
              "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "4  0.012917  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "        101       102       104  ...     youre  yourself1     youve      zaki  \\\n",
              "0  0.000000  0.012966  0.012966  ...  0.000000   0.000000  0.000000  0.006483   \n",
              "1  0.022897  0.000000  0.000000  ...  0.000000   0.000000  0.000000  0.000000   \n",
              "2  0.000000  0.000000  0.000000  ...  0.016476   0.000000  0.000000  0.000000   \n",
              "3  0.000000  0.000000  0.000000  ...  0.070943   0.000000  0.000000  0.000000   \n",
              "4  0.000000  0.000000  0.000000  ...  0.000000   0.004306  0.004306  0.000000   \n",
              "\n",
              "      zaxis   zendesk     zero     zeror  zerovariance       zip  \n",
              "0  0.000000  0.000000  0.01945  0.006483      0.006483  0.000000  \n",
              "1  0.000000  0.000000  0.00000  0.000000      0.000000  0.000000  \n",
              "2  0.000000  0.000000  0.00000  0.000000      0.000000  0.020421  \n",
              "3  0.000000  0.000000  0.00000  0.000000      0.000000  0.000000  \n",
              "4  0.004306  0.004306  0.00000  0.000000      0.000000  0.000000  \n",
              "\n",
              "[5 rows x 2926 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-887bc486-82a7-4b4c-b1c9-a275305f8f87\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>02</th>\n",
              "      <th>048</th>\n",
              "      <th>081</th>\n",
              "      <th>0once</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>104</th>\n",
              "      <th>...</th>\n",
              "      <th>youre</th>\n",
              "      <th>yourself1</th>\n",
              "      <th>youve</th>\n",
              "      <th>zaki</th>\n",
              "      <th>zaxis</th>\n",
              "      <th>zendesk</th>\n",
              "      <th>zero</th>\n",
              "      <th>zeror</th>\n",
              "      <th>zerovariance</th>\n",
              "      <th>zip</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005231</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012966</td>\n",
              "      <td>0.012966</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.01945</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022897</td>\n",
              "      <td>0.022897</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018473</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022897</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020421</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020421</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016476</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070943</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.012917</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2926 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-887bc486-82a7-4b4c-b1c9-a275305f8f87')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-887bc486-82a7-4b4c-b1c9-a275305f8f87 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-887bc486-82a7-4b4c-b1c9-a275305f8f87');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(cosine_similarity(df,dense_output=True))\n",
        "df2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qcYEZvw0nWBF",
        "outputId": "62e439b7-62e8-46c9-b991-25e75f26e2a8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4\n",
              "0  1.000000  0.157824  0.282916  0.329852  0.207403\n",
              "1  0.157824  1.000000  0.079420  0.104079  0.062893\n",
              "2  0.282916  0.079420  1.000000  0.255659  0.106212\n",
              "3  0.329852  0.104079  0.255659  1.000000  0.157657\n",
              "4  0.207403  0.062893  0.106212  0.157657  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e618d41f-9422-4717-be89-d35767be870b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.157824</td>\n",
              "      <td>0.282916</td>\n",
              "      <td>0.329852</td>\n",
              "      <td>0.207403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.157824</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.079420</td>\n",
              "      <td>0.104079</td>\n",
              "      <td>0.062893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.282916</td>\n",
              "      <td>0.079420</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.255659</td>\n",
              "      <td>0.106212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.329852</td>\n",
              "      <td>0.104079</td>\n",
              "      <td>0.255659</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.157657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.207403</td>\n",
              "      <td>0.062893</td>\n",
              "      <td>0.106212</td>\n",
              "      <td>0.157657</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e618d41f-9422-4717-be89-d35767be870b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e618d41f-9422-4717-be89-d35767be870b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e618d41f-9422-4717-be89-d35767be870b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = []\n",
        "for j,k in enumerate(df2.values):\n",
        "  for n in range(len(k)):\n",
        "    t.append([j,n,k[n]])\n",
        "\n",
        "qq = []\n",
        "for i in range(len(t)):\n",
        "  if t[i][0] == t[i][1]:\n",
        "    qq.append([t[i][0],t[i][1],0])\n",
        "  else:\n",
        "    qq.append(t[i])\n",
        "qq[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E5rKCa4ooZb",
        "outputId": "e30025cc-964a-4d1c-ad24-2fa56a9e9469"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0],\n",
              " [0, 1, 0.1578236771958124],\n",
              " [0, 2, 0.28291575578017153],\n",
              " [0, 3, 0.3298515712309559],\n",
              " [0, 4, 0.20740295422415944]]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "u=defaultdict(list)\n",
        "\n",
        "for i in range(len(qq)):\n",
        "  u[qq[i][0]].append(qq[i][2])\n",
        "updated_df = pd.DataFrame(u)\n",
        "\n",
        "position_maxVal=[]\n",
        "for i in range(len(updated_df)):\n",
        "  position_maxVal.append(np.argmax(updated_df[i]))"
      ],
      "metadata": {
        "id": "dv2xFJQUsg4S"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_comp = []\n",
        "for j in position_maxVal:\n",
        "  sent_comp.append(texto[j])\n",
        "similar_texto = pd.DataFrame(sent_comp,columns=['Similar Texto'])\n",
        "similarity_value = pd.DataFrame(round(updated_df.max(axis=1),4),columns=['Similarity Value'])\n",
        "similarity_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lliKFzXusHTc",
        "outputId": "0c91d3fe-d2da-4a4f-d8a0-724f7cb3427b"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Similarity Value\n",
              "0            0.3299\n",
              "1            0.1578\n",
              "2            0.2829\n",
              "3            0.3299\n",
              "4            0.2074"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8d6e152-e244-48dd-bd00-03524c49d246\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.3299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.3299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.2074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8d6e152-e244-48dd-bd00-03524c49d246')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8d6e152-e244-48dd-bd00-03524c49d246 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8d6e152-e244-48dd-bd00-03524c49d246');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_texto = pd.DataFrame(texto,columns=['Parsed Texto'])\n",
        "cos_sim_df = pd.concat([p_texto,similar_texto,similarity_value],axis=1)\n",
        "cos_sim_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VDknikrUvMNE",
        "outputId": "e8a78d82-0a76-43d1-b6b5-e655255d2c3a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Parsed Texto  \\\n",
              "0  section discuss various machine learning algor...   \n",
              "1  interpretable ml developed explain blackbox mo...   \n",
              "2  neural networksneural networks reflect behavio...   \n",
              "3  artificial intelligence overviewin basic terms...   \n",
              "4  const tundefinedtypeof htmlimageelementloading...   \n",
              "\n",
              "                                       Similar Texto  Similarity Value  \n",
              "0  artificial intelligence overviewin basic terms...            0.3299  \n",
              "1  section discuss various machine learning algor...            0.1578  \n",
              "2  section discuss various machine learning algor...            0.2829  \n",
              "3  section discuss various machine learning algor...            0.3299  \n",
              "4  section discuss various machine learning algor...            0.2074  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41066b05-096a-42e1-9bc4-b44977c0bebf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Parsed Texto</th>\n",
              "      <th>Similar Texto</th>\n",
              "      <th>Similarity Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>section discuss various machine learning algor...</td>\n",
              "      <td>artificial intelligence overviewin basic terms...</td>\n",
              "      <td>0.3299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>interpretable ml developed explain blackbox mo...</td>\n",
              "      <td>section discuss various machine learning algor...</td>\n",
              "      <td>0.1578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neural networksneural networks reflect behavio...</td>\n",
              "      <td>section discuss various machine learning algor...</td>\n",
              "      <td>0.2829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>artificial intelligence overviewin basic terms...</td>\n",
              "      <td>section discuss various machine learning algor...</td>\n",
              "      <td>0.3299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>const tundefinedtypeof htmlimageelementloading...</td>\n",
              "      <td>section discuss various machine learning algor...</td>\n",
              "      <td>0.2074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41066b05-096a-42e1-9bc4-b44977c0bebf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41066b05-096a-42e1-9bc4-b44977c0bebf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41066b05-096a-42e1-9bc4-b44977c0bebf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cos_sim_df['Parsed Texto'][1])\n",
        "print('----------------------------------')\n",
        "print(cos_sim_df['Similar Texto'][1])\n",
        "print('----------------------------------')\n",
        "print('Calculo da Similaridade:',cos_sim_df['Similarity Value'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaoIAVJYv2EN",
        "outputId": "c9716c77-06d5-4352-fa9b-f40f5c36ca2e"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "interpretable ml developed explain blackbox models 10 heterogeneous etiology ad completely understood yet interpretability important enables validation biological plausibility ml models recently studies used interpretable ml ad detectionfor example long shortterm memory lstm 11 based recurrent neural networks rnn 12 trained classify cn vs mci subjects 13 experiments included multiple techniques fuse sociodemographic genetic data magnetic resonance imaging mri scans resulting models evaluated two ad datasetsthe ad subset 14 heinz nixdorf risk factors evaluation coronary calcification lifestyle recall hnr 15 61 mci 59 cn 624 subjects 397 mci 227 cn alzheimers disease neuroimaging initiative adni 16 study phase 1 visually explain individual model decisions gradientweighted class activation mapping gradcam 17 used focus biologically plausible regions observedfour heatmap visualization methodssensitivity analysis 18 guided backpropagation 19 occlusion 20 brain area occlusion inspired 21 compared 3dcnns 22 cnn models trained using 969 mri scans 344 adni subjects 151 cn 193 ad however unclear whether described workflow ensured independent training test sets using multiple scans per subject 23 thus crossvalidation cv accuracy 77 pm 6 might affected data leakage heatmaps focused adrelated anatomical brain areasan interpretable deep learning model consisting generative adversarial network 24 extend training dataset regression network generate feature vectors adjacent visits classification model introduced 25 first regression model iteratively estimated feature vector following visit resulting feature vector used input classification model predicted final diagnosis classify 101 pmci vs 115 smci adni subjects longitudinal volumetric mri features used model outperformed svms artificial neural networksa new interpretable model based distinct weighted rules introduced 26 evaluated 151 subjects 97 ad 54 cn adni cohort framework called sparse highorder interaction model rejection option shimr consists two hierarchical stages first stage interpretable model trained using plasma features data subjects unclear prediction stage propagated second stage stage svm 9 trained using invasive cerebrospinal fluid csf markers evaluation included cv independent test set described model reached area receiveroperating characteristics curve auroc 081 test setshapley additive explanations shap 27 used 28 explain differences models trained using coreset selection methods idea determine coresets subjects informative data rf xgboost models trained coresets avoid overfitting improve ml models results data shapley 29 coreset selection compared leaveoneout 30 selection random exclusion models trained validated adni dataset 400 smci 319 pmci externally validated subset aibl dataset 16 smci 12 pmci shap summary plots showed models trained entire training set coreset learned biologically plausible associationsto examine predictive influence upbeta amyloid plaques tau tangles neurodegeneration disease progression rf feature importance used 31 experimental data included 405 adni subjects 148 cn 147 mci 110 ad upbeta amyloid positron emission tomography pet detected upbeta amyloid plaques invasive csf features surrogated tau tangles mri fluorodeoxyglucose fdg pet scans used determine neurodegeneration experimental results showed models trained classify early ad stages preferred features representing tau tangles upbeta amyloid plaques models trained predict later stages favored surrogates neurodegeneration shap 27 gradient tree boosting gtb 32 reproduced observations rf entire feature set reached accuracies 7317 cn vs mci 7101 mci vs ad 9034 cn vs ad shap values also used 33 explain populationbased individual predictions xgboost models rfs models trained using sociodemographic lifestyle factors predict patients risk develop ad based medical history transfer learning applied information extracted survey health ageing retirement europe share 34 80 699 cn 4 157 ad prevent cohort 35 109 subjects high risk develop ad 364 subjects low risk prevent cohort younger share cohort models support hypothesis age important risk factor ad detection consistent previous research 36 among factors less education physical inactivity diabetes infrequent social contact identified potential risk factorsa two stagebased classification workflow used shap values interpret rfs developed 37 first stage cn vs mci vs ad classification performed second stage implemented differentiation smci pmci subjects models based multiple modalities including mri pet csf biomarkers cognitive tests medical history genetics many rfs trained tested using 1 048 subjects 294 cn 254 smci 232 pmci 268 ad adni dataset cn vs mci vs ad classification model almost exclusively selected cognitive test scores important features model learned bad cognitive test results increased risk ad mci important features smci vs pmci classification also cognitive test scores followed pet mri features bad cognitive test scores small mri volumes small pet uptakes associated disease progression table 1 table 1 summary related workfull size table\n",
            "----------------------------------\n",
            "section discuss various machine learning algorithms include classification analysis regression analysis data clustering association rule learning feature engineering dimensionality reduction well deep learning methods general structure machine learningbased predictive model shown fig 3 model trained historical data phase 1 outcome generated phase 2 new test datafig 3a general structure machine learning based predictive model considering training testing phasefull size imageclassification analysisclassification regarded supervised learning method machine learning referring problem predictive modeling well class label predicted given example 41 mathematically maps function f input variables x output variables target label categories predict class given data points carried structured unstructured data example spam detection spam spam email service providers classification problem following summarize common classification problemsbinary classification refers classification tasks two class labels true false yes 41 binary classification tasks one class could normal state abnormal state could another class instance cancer detected normal state task involves medical test cancer detected could considered abnormal state similarly spam spam example email service providers considered binary classificationmulticlass classification traditionally refers classification tasks two class labels 41 multiclass classification principle normal abnormal outcomes unlike binary classification tasks instead within range specified classes examples classified belonging one example multiclass classification task classify various types network attacks nslkdd 119 dataset attack categories classified four class labels dos denial service attack u2r user root attack r2l root local attack probing attackmultilabel classification machine learning multilabel classification important consideration example associated several classes labels thus generalization multiclass classification classes involved problem hierarchically structured example may simultaneously belong one class hierarchical level eg multilevel text classification instance google news presented categories city name technology latest news etc multilabel classification includes advanced machine learning algorithms support predicting various mutually nonexclusive classes labels unlike traditional classification tasks class labels mutually exclusive 82 many classification algorithms proposed machine learning data science literature 41 125 following summarize common popular methods used widely various application areasnaive bayes nb naive bayes algorithm based bayes theorem assumption independence pair features 51 works well used binary multiclass categories many realworld situations document text classification spam filtering etc effectively classify noisy instances data construct robust prediction model nb classifier used 94 key benefit compared sophisticated approaches needs small amount training data estimate necessary parameters quickly 82 however performance may affect due strong assumptions features independence gaussian multinomial complement bernoulli categorical common variants nb classifier 82 linear discriminant analysis lda linear discriminant analysis lda linear decision boundary classifier created fitting class conditional densities data applying bayes rule 51 82 method also known generalization fishers linear discriminant projects given dataset lowerdimensional space ie reduction dimensionality minimizes complexity model reduces resulting models computational costs standard lda model usually suits class gaussian density assuming classes share covariance matrix 82 lda closely related anova analysis variance regression analysis seek express one dependent variable linear combination features measurementslogistic regression lr another common probabilistic based statistical model used solve classification issues machine learning logistic regression lr 64 logistic regression typically uses logistic function estimate probabilities also referred mathematically defined sigmoid function eq 1 overfit highdimensional datasets works well dataset separated linearly regularization l1 l2 techniques 82 used avoid overfitting scenarios assumption linearity dependent independent variables considered major drawback logistic regression used classification regression problems commonly used classification begin aligned g z frac 1 1 + exp z end aligned 1 knearest neighbors knn knearest neighbors knn 9 instancebased learning nongeneralizing learning also known lazy learning algorithm focus constructing general internal model instead stores instances corresponding training data ndimensional space knn uses data classifies new data points based similarity measures eg euclidean distance function 82 classification computed simple majority vote k nearest neighbors point quite robust noisy training data accuracy depends data quality biggest issue knn choose optimal number neighbors considered knn used classification well regressionsupport vector machine svm machine learning another common technique used classification regression tasks support vector machine svm 56 high infinitedimensional space support vector machine constructs hyperplane set hyperplanes intuitively hyperplane greatest distance nearest training data points class achieves strong separation since general greater margin lower classifiers generalization error effective highdimensional spaces behave differently based different mathematical functions known kernel linear polynomial radial basis function rbf sigmoid etc popular kernel functions used svm classifier 82 however data set contains noise overlapping target classes svm perform welldecision tree dt decision tree dt 88 wellknown nonparametric supervised learning method dt learning methods used classification regression tasks 82 id3 87 c45 88 cart 20 well known dt algorithms moreover recently proposed behavdt 100 intrudtree 97 sarker et al effective relevant application domains user behavior analytics cybersecurity analytics respectively sorting tree root leaf nodes shown fig 4 dt classifies instances instances classified checking attribute defined node starting root node tree moving tree branch corresponding attribute value splitting popular criteria gini gini impurity entropy information gain expressed mathematically 82 begin aligned mathrm entropy h x sum _ 1 n p x_i log _2 p x_i end aligned 2 begin aligned mathrm gini e 1 sum _ 1 c p_i 2 end aligned 3 fig 4an example decision tree structurefull size imagefig 5an example random forest structure considering multiple decision treesfull size imagerandom forest rf random forest classifier 19 well known ensemble classification technique used field machine learning data science various application areas method uses parallel ensembling fits several decision tree classifiers parallel shown fig 5 different data set subsamples uses majority voting averages outcome final result thus minimizes overfitting problem increases prediction accuracy control 82 therefore rf learning model multiple decision trees typically accurate single decision tree based model 106 build series decision trees controlled variation combines bootstrap aggregation bagging 18 random feature selection 11 adaptable classification regression problems fits well categorical continuous valuesadaptive boosting adaboost adaptive boosting adaboost ensemble learning process employs iterative approach improve poor classifiers learning errors developed yoav freund et al 35 also known metalearning unlike random forest uses parallel ensembling adaboost uses sequential ensembling creates powerful classifier combining many poorly performing classifiers obtain good classifier high accuracy sense adaboost called adaptive classifier significantly improving efficiency classifier instances trigger overfits adaboost best used boost performance decision trees base estimator 82 binary classification problems however sensitive noisy data outliersextreme gradient boosting xgboost gradient boosting like random forests 19 ensemble learning algorithm generates final model based series individual models typically decision trees gradient used minimize loss function similar neural networks 41 use gradient descent optimize weights extreme gradient boosting xgboost form gradient boosting takes detailed approximations account determining best model 82 computes secondorder gradients loss function minimize loss advanced regularization l1 l2 82 reduces overfitting improves model generalization performance xgboost fast interpret handle largesized datasets wellstochastic gradient descent sgd stochastic gradient descent sgd 41 iterative method optimizing objective function appropriate smoothness properties word stochastic refers random probability reduces computational burden particularly highdimensional optimization problems allowing faster iterations exchange lower convergence rate gradient slope function calculates variables degree change response another variables changes mathematically gradient descent convex function whose output partial derivative set input parameters let alpha learning rate j_i training example cost mathrm th eq 4 represents stochastic gradient descent weight update method jmathrm th iteration largescale sparse machine learning sgd successfully applied problems often encountered text classification natural language processing 82 however sgd sensitive feature scaling needs range hyperparameters regularization parameter number iterations begin aligned w_j w_j alpha frac partial j_i partial w_j end aligned 4 rulebased classification term rulebased classification used refer classification scheme makes use ifthen rules class prediction several classification algorithms zeror 125 oner 47 decision trees 87 88 dtnb 110 ripple rule learner ridor 125 repeated incremental pruning produce error reduction ripper 126 exist ability rule generation decision tree one common rulebased classification algorithms among techniques several advantages easier interpret ability handle highdimensional data simplicity speed good accuracy capability produce rules human clear understandable classification 127 128 decision treebased rules also provide significant accuracy prediction model unseen test cases 106 since rules easily interpretable rulebased classifiers often used produce descriptive models describe system including entities relationshipsfig 6classification vs regression classification dotted line represents linear boundary separates two classes regression dotted line models linear relationship two variablesfull size imageregression analysisregression analysis includes several methods machine learning allow predict continuous result variable based value one x predictor variables 41 significant distinction classification regression classification predicts distinct class labels regression facilitates prediction continuous quantity figure 6 shows example classification different regression models overlaps often found two types machine learning algorithms regression models widely used variety fields including financial forecasting prediction cost estimation trend analysis marketing time series estimation drug response modeling many familiar types regression algorithms linear polynomial lasso ridge regression etc explained briefly followingsimple multiple linear regression one popular ml modeling techniques well wellknown regression technique technique dependent variable continuous independent variable continuous discrete form regression line linear linear regression creates relationship dependent variable one independent variables x also known regression line using best fit straight line 41 defined following equations begin aligned + bx + e end aligned 5 begin aligned + b_1x_1 + b_2x_2 + cdots + b_nx_n + e end aligned 6 intercept b slope line e error term equation used predict value target variable based given predictor variable multiple linear regression extension simple linear regression allows two predictor variables model response variable linear function 41 defined eq 6 whereas simple linear regression 1 independent variable defined eq 5polynomial regression polynomial regression form regression analysis relationship independent variable x dependent variable linear polynomial degree nmathrm th x 82 equation polynomial regression also derived linear regression polynomial regression degree 1 equation defined begin aligned b_0 + b_1x + b_2x2 + b_3x3 + cdots + b_nxn + e end aligned 7 predicted target output b_0 b_1 b_n regression coefficients x independent input variable simple words say data distributed linearly instead nmathrm th degree polynomial use polynomial regression get desired outputlasso ridge regression lasso ridge regression well known powerful techniques typically used building learning models presence large number features due capability preventing overfitting reducing complexity model lasso least absolute shrinkage selection operator regression model uses l1 regularization technique 82 uses shrinkage penalizes absolute value magnitude coefficients l1 penalty result lasso appears render coefficients absolute zero thus lasso regression aims find subset predictors minimizes prediction error quantitative response variable hand ridge regression uses l2 regularization 82 squared magnitude coefficients l2 penalty thus ridge regression forces weights small never sets coefficient value zero nonsparse solution overall lasso regression useful obtain subset predictors eliminating less important features ridge regression useful data set multicollinearity refers predictors correlated predictorscluster analysiscluster analysis also known clustering unsupervised machine learning technique identifying grouping related data points large datasets without concern specific outcome grouping collection objects way objects category called cluster sense similar objects groups 41 often used data analysis technique discover interesting trends patterns data eg groups consumers based behavior broad range application areas cybersecurity ecommerce mobile data processing health analytics user modeling behavioral analytics clustering used following briefly discuss summarize various types clustering methodspartitioning methods based features similarities data clustering approach categorizes data multiple groups clusters data scientists analysts typically determine number clusters either dynamically statically depending nature target applications produce methods clustering common clustering algorithms based partitioning methods kmeans 69 kmediods 80 clara 55 etcdensitybased methods identify distinct groups clusters uses concept cluster data space contiguous region high point density isolated clusters contiguous regions low point density points part cluster considered noise typical clustering algorithms based density dbscan 32 optics 12 etc densitybased methods typically struggle clusters similar density high dimensionality datahierarchicalbased methods hierarchical clustering typically seeks construct hierarchy clusters ie tree structure strategies hierarchical clustering generally fall two types agglomerativea bottomup approach observation begins cluster pairs clusters combined one moves hierarchy ii divisivea topdown approach observations begin one cluster splits performed recursively moves hierarchy shown fig 7 earlier proposed bots technique sarker et al 102 example hierarchical particularly bottomup clustering algorithmgridbased methods deal massive datasets gridbased clustering especially suitable obtain clusters principle first summarize dataset grid representation combine grid cells sting 122 clique 6 etc standard algorithms gridbased clusteringmodelbased methods mainly two types modelbased clustering algorithms one uses statistical learning based method neural network learning 130 instance gmm 89 example statistical learning method som 22 96 example neural network learning methodconstraintbased methods constrainedbased clustering semisupervised approach data clustering uses constraints incorporate domain knowledge application useroriented constraints incorporated perform clustering typical algorithms kind clustering cop kmeans 121 cmwkmeans 27 etcfig 7a graphical interpretation widelyused hierarchical clustering bottomup topdown techniquefull size imagemany clustering algorithms proposed ability grouping data machine learning data science literature 41 125 following summarize popular methods used widely various application areaskmeans clustering kmeans clustering 69 fast robust simple algorithm provides reliable results data sets wellseparated data points allocated cluster algorithm way amount squared distance data points centroid small possible words kmeans algorithm identifies k number centroids assigns data point nearest cluster keeping centroids small possible since begins random selection cluster centers results inconsistent since extreme values easily affect mean kmeans clustering algorithm sensitive outliers kmedoids clustering 91 variant kmeans robust noises outliersmeanshift clustering meanshift clustering 37 nonparametric clustering technique require prior knowledge number clusters constraints cluster shape meanshift clustering aims discover blobs smooth distribution density samples 82 centroidbased algorithm works updating centroid candidates mean points given region form final set centroids candidates filtered postprocessing stage remove nearduplicates cluster analysis computer vision image processing examples application domains mean shift disadvantage computationally expensive moreover cases high dimension number clusters shifts abruptly meanshift algorithm work welldbscan densitybased spatial clustering applications noise dbscan 32 base algorithm densitybased clustering widely used data mining machine learning known nonparametric densitybased clustering technique separating highdensity clusters lowdensity clusters used model building dbscans main idea point belongs cluster close many points cluster find clusters various shapes sizes vast volume data noisy contains outliers dbscan unlike kmeans require priori specification number clusters data find arbitrarily shaped clusters although kmeans much faster dbscan efficient finding highdensity regions outliers ie robust outliersgmm clustering gaussian mixture models gmms often used data clustering distributionbased clustering algorithm gaussian mixture model probabilistic model data points produced mixture finite number gaussian distributions unknown parameters 82 find gaussian parameters cluster optimization algorithm called expectationmaximization em 82 used em iterative method uses statistical model estimate parameters contrast kmeans gaussian mixture models account uncertainty return likelihood data point belongs one k clusters gmm clustering robust kmeans works well even nonlinear data distributionsagglomerative hierarchical clustering common method hierarchical clustering used group objects clusters based similarity agglomerative clustering technique uses bottomup approach object first treated singleton cluster algorithm following pairs clusters merged one one clusters merged single large cluster containing objects result dendrogram treebased representation elements single linkage 115 complete linkage 116 bots 102 etc examples techniques main advantage agglomerative hierarchical clustering kmeans treestructure hierarchy generated agglomerative clustering informative unstructured collection flat clusters returned kmeans help make better decisions relevant application areasdimensionality reduction feature learningin machine learning data science highdimensional data processing challenging task researchers application developers thus dimensionality reduction unsupervised learning technique important leads better human interpretations lower computational costs avoids overfitting redundancy simplifying models process feature selection feature extraction used dimensionality reduction primary distinction selection extraction features feature selection keeps subset original features 97 feature extraction creates brand new ones 98 following briefly discuss techniquesfeature selection selection features also known selection variables attributes data process choosing subset unique features variables predictors use building machine learning data science model decreases models complexity eliminating irrelevant less important features allows faster training machine learning algorithms right optimal subset selected features problem domain capable minimize overfitting problem simplifying generalizing model well increases models accuracy 97 thus feature selection 66 99 considered one primary concepts machine learning greatly affects effectiveness efficiency target machine learning model chisquared test analysis variance anova test pearsons correlation coefficient recursive feature elimination popular techniques used feature selectionfeature extraction machine learningbased model system feature extraction techniques usually provide better understanding data way improve prediction accuracy reduce computational cost training time aim feature extraction 66 99 reduce number features dataset generating new ones existing ones discarding original features majority information found original set features summarized using new reduced set features instance principal components analysis pca often used dimensionalityreduction technique extract lowerdimensional space creating new brand components existing features dataset 98 many algorithms proposed reduce data dimensions machine learning data science literature 41 125 following summarize popular methods used widely various application areasvariance threshold simple basic approach feature selection variance threshold 82 excludes features low variance ie features whose variance exceed threshold eliminates zerovariance characteristics default ie characteristics value samples feature selection algorithm looks x features outputs needed therefore used unsupervised learningpearson correlation pearsons correlation another method understand features relation response variable used feature selection 99 method also used finding association features dataset resulting value 1 1 1 means perfect negative correlation +1 means perfect positive correlation 0 means two variables linear correlation two random variables represent x correlation coefficient x defined 41 begin aligned r x frac sum _ 1 n x_i bar x y_i bar sqrt sum _ 1 n x_i bar x 2 sqrt sum _ 1 n y_i bar 2 end aligned 8 anova analysis variance anova statistical tool used verify mean values two groups differ significantly anova assumes linear relationship variables target variables normal distribution statistically test equality means anova method utilizes f tests feature selection results anova f value 82 test used certain features independent goal variable omittedchi square chisquare chi 2 82 statistic estimate difference effects series events variables observed expected frequencies magnitude difference real observed values degrees freedom sample size depends chi 2 chisquare chi 2 commonly used testing relationships categorical variables o_i represents observed value e_i represents expected value begin aligned chi 2 sum _ 1 n frac o_i e_i 2 e_i end aligned 9 recursive feature elimination rfe recursive feature elimination rfe brute force approach feature selection rfe 82 fits model removes weakest feature meets specified number features features ranked coefficients feature significance model rfe aims remove dependencies collinearity model recursively removing small number features per iterationmodelbased selection reduce dimensionality data linear models penalized l1 regularization used least absolute shrinkage selection operator lasso regression type linear regression property shrinking coefficients zero 82 therefore feature removed model thus penalized lasso regression method often used machine learning select subset variables extra trees classifier 82 example treebased estimator used compute impuritybased function importance used discard irrelevant featuresprincipal component analysis pca principal component analysis pca wellknown unsupervised learning approach field machine learning data science pca mathematical technique transforms set correlated variables set uncorrelated variables known principal components 48 81 figure 8 shows example effect pca various dimensions space fig 8a shows original features 3d space fig 8b shows created principal components pc1 pc2 onto 2d plane 1d line principal component pc1 respectively thus pca used feature extraction technique reduces dimensionality datasets build effective machine learning model 98 technically pca identifies completely transformed highest eigenvalues covariance matrix uses project data new subspace equal fewer dimensions 82 fig 8an example principal component analysis pca created principal components pc1 pc2 different dimension spacefull size imageassociation rule learningassociation rule learning rulebased machine learning approach discover interesting relationships ifthen statements large datasets variables 7 one example customer buys computer laptop item likely also buy antivirus software another item time association rules employed today many application areas including iot services medical diagnosis usage behavior analytics web usage mining smartphone applications cybersecurity applications bioinformatics comparison sequence mining association rule learning usually take account order things within across transactions common way measuring usefulness association rules use parameter support confidence introduced 7 data mining literature many association rule learning methods proposed logic dependent 34 frequent pattern based 8 49 68 treebased 42 popular association rule learning algorithms summarized belowais setm ais first algorithm proposed agrawal et al 7 association rule mining ais algorithms main downside many candidate itemsets generated requiring space wasting lot effort algorithm calls many passes entire dataset produce rules another approach setm 49 exhibits good performance stable behavior execution time however suffers flaw ais algorithmapriori generating association rules given dataset agrawal et al 8 proposed apriori aprioritid apriorihybrid algorithms later algorithms outperform ais setm mentioned due apriori property frequent itemset 8 term apriori usually refers prior knowledge frequent itemset properties apriori uses bottomup approach generates candidate itemsets reduce search space apriori uses property subsets frequent itemset must frequent itemset infrequent supersets must also infrequent another approach predictive apriori 108 also generate rules however receives unexpected results combines support confidence apriori 8 widely applicable techniques mining association ruleseclat technique proposed zaki et al 131 stands equivalence class clustering bottomup lattice traversal eclat uses depthfirst search find frequent itemsets contrast apriori 8 algorithm represents data horizontal pattern represents data vertically hence eclat algorithm efficient scalable area association rule learning algorithm better suited small medium datasets whereas apriori algorithm used large datasetsfpgrowth another common association rule learning technique based frequentpattern tree fptree proposed han et al 42 frequent pattern growth known fpgrowth key difference apriori generating rules apriori algorithm 8 generates frequent candidate itemsets hand fpgrowth algorithm 42 prevents candidate generation thus produces tree successful strategy divide conquer approach due sophistication however fptree challenging use interactive mining environment 133 thus fptree would fit memory massive data sets making challenging process big data well another solution rarm rapid association rule mining proposed das et al 26 faces related fptree issue 133 abcruleminer rulebased machine learning method recently proposed earlier paper sarker et al 104 discover interesting nonredundant rules provide realworld intelligent services algorithm effectively identifies redundancy associations taking account impact precedence related contextual features discovers set nonredundant association rules algorithm first constructs association generation tree agt topdown approach extracts association rules traversing tree thus abcruleminer potent traditional rulebased methods terms nonredundant rule generation intelligent decisionmaking particularly contextaware smart computing environment human user preferences involvedamong association rule learning techniques discussed apriori 8 widely used algorithm discovering association rules given dataset 133 main strength association learning technique comprehensiveness generates associations satisfy userspecified constraints minimum support confidence value abcruleminer approach 104 discussed earlier could give significant results terms nonredundant rule generation intelligent decisionmaking relevant application areas real worldreinforcement learningreinforcement learning rl machine learning technique allows agent learn trial error interactive environment using input actions experiences unlike supervised learning based given sample data examples rl method based interacting environment problem solved reinforcement learning rl defined markov decision process mdp 86 ie sequentially making decisions rl problem typically includes four elements agent environment rewards policyrl split roughly modelbased modelfree techniques modelbased rl process inferring optimal behavior model environment performing actions observing results include next state immediate reward 85 alphazero alphago 113 examples modelbased approaches hand modelfree approach use distribution transition probability reward function associated mdp qlearning deep q network monte carlo control sarsa stateactionrewardstateaction etc examples modelfree algorithms 52 policy network required modelbased rl modelfree key difference modelfree modelbased learning following discuss popular rl algorithmsmonte carlo methods monte carlo techniques monte carlo experiments wide category computational algorithms rely repeated random sampling obtain numerical results 52 underlying concept use randomness solve problems deterministic principle optimization numerical integration making drawings probability distribution three problem classes monte carlo techniques commonly usedqlearning qlearning modelfree reinforcement learning algorithm learning quality behaviors tell agent action take conditions 52 need model environment hence term modelfree deal stochastic transitions rewards without need adaptations q qlearning usually stands quality algorithm calculates maximum expected rewards given behavior given statedeep qlearning basic working step deep qlearning 52 initial state fed neural network returns qvalue possible actions output still reasonably simple setting overcome qlearning works well however number states actions becomes complicated deep learning used function approximatorreinforcement learning along supervised unsupervised learning one basic machine learning paradigms rl used solve numerous realworld problems various fields game theory control theory operations analysis information theory simulationbased optimization manufacturing supply chain logistics multiagent systems swarm intelligence aircraft control robot motion control many moreartificial neural network deep learningdeep learning part wider family artificial neural networks ann based machine learning approaches representation learning deep learning provides computational architecture combining several processing layers input hidden output layers learn data 41 main advantage deep learning traditional machine learning methods better performance several cases particularly learning large datasets 105 129 figure 9 shows general performance deep learning machine learning considering increasing amount datahowever may vary depending data characteristics experimental set upfig 9machine learning deep learning performance general amount datafull size imagethe common deep learning algorithms multilayer perceptron mlp convolutional neural network cnn convnet long shortterm memory recurrent neural network lstmrnn 96 following discuss various types deep learning methods used build effective datadriven models various purposesfig 10a structure artificial neural network modeling multiple processing layersfull size imagemlp base architecture deep learning also known feedforward artificial neural network called multilayer perceptron mlp 82 typical mlp fully connected network consisting input layer one hidden layers output layer shown fig 10 node one layer connects node following layer certain weight mlp utilizes backpropagation technique 41 fundamental building block neural network adjust weight values internally building model mlp sensitive scaling features allows variety hyperparameters tuned number hidden layers neurons iterations result computationally costly modelcnn convnet convolution neural network cnn 65 enhances design standard ann consisting convolutional layers pooling layers well fully connected layers shown fig 11 takes advantage twodimensional 2d structure input data typically broadly used several areas image video recognition image processing classification medical image analysis natural language processing etc cnn greater computational burden without manual intervention advantage automatically detecting important features hence cnn considered powerful conventional ann number advanced deep learning models based cnn used field alexnet 60 xception 24 inception 118 visual geometry group vgg 44 resnet 45 etclstmrnn long shortterm memory lstm artificial recurrent neural network rnn architecture used area deep learning 38 lstm feedback links unlike normal feedforward neural networks lstm networks wellsuited analyzing learning sequential data classifying processing predicting data based time series data differentiates conventional networks thus lstm used data sequential format time sentence etc commonly applied area timeseries analysis natural language processing speech recognition etcfig 11an example convolutional neural network cnn convnet including multiple convolution pooling layersfull size imagein addition common deep learning methods discussed several deep learning approaches 96 exist area various purposes instance selforganizing map som 58 uses unsupervised learning represent highdimensional data 2d grid map thus achieving dimensionality reduction autoencoder ae 15 another learning technique widely used dimensionality reduction well feature extraction unsupervised learning tasks restricted boltzmann machines rbm 46 used dimensionality reduction classification regression collaborative filtering feature learning topic modeling deep belief network dbn typically composed simple unsupervised networks restricted boltzmann machines rbms autoencoders backpropagation neural network bpnn 123 generative adversarial network gan 39 form network deep learning generate data characteristics close actual data input transfer learning currently common train deep neural networks comparatively low data typically reuse new problem pretrained model 124 brief discussion artificial neural networks ann deep learning dl models summarized earlier paper sarker et al 96 overall based learning techniques discussed conclude various types machine learning techniques classification analysis regression data clustering feature selection extraction dimensionality reduction association rule learning reinforcement learning deep learning techniques play significant role various purposes according capabilities following section discuss several application areas based machine learning algorithms\n",
            "----------------------------------\n",
            "Calculo da Similaridade: 0.1578\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b82764f8568d3e731c1a8fca5041e2084a073129ef40aeb754987146397fb09b"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}